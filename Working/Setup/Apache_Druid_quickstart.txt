Druid Quickstart setup
--------------------
--Setup 1 linux machine in vmbox/your own linux/cloud platform
--java installed (openjdk8)[refer java_setup file if needed]

if logged in as root or
$sudo su
<pswd>

--Download druid:
root@m1:~# wget https://ftp-stud.hs-esslingen.de/pub/Mirrors/ftp.apache.org/dist/druid/0.21.1/apache-druid-0.21.1-bin.tar.gz
--2021-06-13 13:12:38--  https://ftp-stud.hs-esslingen.de/pub/Mirrors/ftp.apache.org/dist/druid/0.21.1/apache-druid-0.21.1-bin.tar.gz
Resolving ftp-stud.hs-esslingen.de (ftp-stud.hs-esslingen.de)... 129.143.116.10, 2001:7c0:700::10
Connecting to ftp-stud.hs-esslingen.de (ftp-stud.hs-esslingen.de)|129.143.116.10|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 374455414 (357M) [application/x-gzip]
Saving to: ‘apache-druid-0.21.1-bin.tar.gz’

apache-druid-0.21.1-bin.tar.gz        100%[======================================================================>] 357.11M   264MB/s    in 1.4s

2021-06-13 13:12:40 (264 MB/s) - ‘apache-druid-0.21.1-bin.tar.gz’ saved [374455414/374455414]
 
root@m1:~# ls
apache-druid-0.21.1-bin.tar.gz

root@m1:~# java -version
openjdk version "1.8.0_292"
OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10)
OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)

root@m1:~# cd /usr/local/
root@m1:/usr/local# tar -xvf /root/apache-druid-0.21.1-bin.tar.gz
root@m1:/usr/local# ln -s apache-druid-0.21.1 druid
root@m1:/usr/local# chown -R hdu:hdu apache-druid-0.21.1/*
root@m1:/usr/local# chown -R hdu:hdu apache-druid-0.21.1*
root@m1:/usr/local# chown -R hdu:hdu druid*

su - hdu
hdu@m1:~$ vi .bashrc
update .bashrc for 'hdu' user
export DRUID_HOME=/usr/local/druid
export PATH=$PATH:$DRUID_HOME/bin
export DRUID_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
export PATH=$PATH:$DRUID_JAVA_HOME/bin

hdu@m1:~$ source .bashrc


to test quickstart of druid:
<logged in as hdu>
hdu@m1:~$ start-micro-quickstart

Access console:
http://<ipaddress:8888>

Note** if we use this single node cluster and later want to clean it up to start fresh
we can delete /usr/local/druid/var

---Testing quickstart
It may take a few seconds for all Druid services to finish starting, 
including the Druid router, which serves the console. 
If you attempt to open the Druid console before startup is complete, 
you may see errors in the browser.

Loading data:
Ingestion specs define the schema of the data Druid reads and stores. 
You can write ingestion specs by hand or using the data loader, 
as we'll do here to perform batch file loading with Druid's native batch ingestion.

The Druid distribution bundles sample data we can use. 
The sample data located in quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz 
in the Druid root directory represents Wikipedia page edits for a given day.

Local disk > 
Base directory: quickstart/tutorial/
File filter: wikiticker-2015-09-12-sampled.json.gz

Apply>
parse data>
The data loader tries to determine the parser appropriate for the data format 
automatically. In this case it identifies the data format as json

With the JSON parser selected, click Next: Parse time. The Parse time settings are 
where you view and adjust the primary timestamp column for the data.

Druid requires data to have a primary timestamp column (internally stored in a column called __time). 
If you do not have a timestamp in your data, select Constant value. 
In our example, the data loader determines that the time column is the only candidate 
that can be used as the primary time column.

Click Next: Transform, Next: Filter, and then Next: Configure schema

The Configure schema settings are where you configure what dimensions and metrics are ingested. 

Since our dataset is very small, you can turn off rollup by unsetting the Rollup 
switch and confirming the change when prompted.

Click Next: Partition to configure how the data will be split into segments. 
In this case, choose DAY as the Segment granularity.

Click Next: Tune and Next: Publish.

The Publish settings are where you specify the datasource name in Druid. 
Let's change the default name from wikiticker-2015-09-12-sampled to wikipedia.

Click Next: Edit spec to review the ingestion spec we've constructed with the data loader.

Submit.

The new task for our wikipedia datasource now appears in the Ingestion view.

The task status should be "SUCCESS", with the duration of the task indicated.

A successful task means that one or more segments have been built and are now picked up by our data servers.

Querying the data:
Click Datasources from the console header.

When the datasource is available, open the Actions menu (Actions) for that datasource 
and choose Query with SQL. 

SELECT * FROM "wikipedia"






